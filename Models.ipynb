{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Препроцессинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import re\n",
    "import pymystem3\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pymystem3 import Mystem\n",
    "import gensim\n",
    "import logging\n",
    "import nltk.data\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тут обработка моих примеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Работа с таблицей по векам\n",
    "\n",
    "wb = openpyxl.load_workbook('/Users/annaaksenova/Desktop/Работа к Кувшинской/То есть выборка по векам.xlsx')\n",
    "sheet = wb['21'] # Имя лист"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = Mystem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Я засунула пластинку и блюдо в один пакет, положила под сиденье и ― надо знать наши самолёты советских времён, лишённые кондиционеров, ― в конце пути в Ереван достала из-под сиденья пластинку, принявшую форму таза, тоесть блюда.  ', ' Само название места происходит от того, что здесь врачевали «косимых» ― тоесть увечных.  ', ' Я даже не могу найти концов, тоесть понять, кто же мне, собственно, должен платить.  ', 'Никаких провокаций ― известные имена-брэнды.  Расчёт на то, что, даже если зритель не шибкоприобщён к балету и не знает, кто такая Аньес Летестю, он непременно отреагирует на Opera National de Paris.  Или хотя бы на «Лебединое озеро». Или хотя бы на Paris.  Плюс гарантия качества: звёзды подобраны заслуженные, но при этом, что важно, действующие: спешно доставленные напароходе прямо из Парижа, поднимешь крышку ― пар! тоесть готовые доказывать свой статус каждым спектаклем.  ', '  «Чтобы не быть выдавленными с рынка, ― говорит академик Алексей Макаров, ― необходимы жесткий расчет экономической эффективности нефтегазовых проектов, контроль затрат по всей цепочке поставок углеводородов, а также правильная оценка возможных рисков, тоесть нужна такая организация бизнес-процессов в отрасли, чтобы перекрывать дорогу чрезмерным затратам».  ', 'Чёткое определение рынков, ориентация на нужды потребителей, координация всех видов маркетинговой деятельности, направленной наудовлетворение потребителей, позволяют компании получать прибыль из создания потребительской ценности и долговременных отношений. тоесть компания отнюдь не должна пытаться дать потребителям абсолютно всё, что они хотят.  ', '  Вика отмерла и начала сбивчиво объяснять про какую-то эсэмэску, которую ей прислала какая-то Лилька и которую, в смысле эсэмэску, необходимо было зачем-то показать директрисе, она и показала, а Лидушка почему-то сразу ей мобильник назад не вернула, а сказала, что вернет его завтра, а назавтра ― это четверг, тоесть позавчера ― Лидушка поперлась с полдня на какое-то совещание, а в пятницу, тоесть вчера, Вика про  мобильник не вспомнила, а потом Лидушку убили.  ', '  Общероссийским классификатором продукции ОК 005-93, утверждённым постановлением Госстандарта России от 30. 12. 1993№ 301 (далее― Классификатор), продукция рыбная, вылов рыбы и других водных биоресурсов (код 989934) отнесены к продукции животноводства, тоесть к сельскохозяйственной продукции.  ', ' Чтобы через много лет люди смотрели на них и вспоминали своих бабушек-дедушек, тоесть нас всех.  ', ' Эти липосомы содержат лиганды к клеточным рецепторам B-клеток (латинская B, «бэ-клетки», они же белые клетки крови ― прим. пресс-службы), тоесть такие молекулы, которые могут взаимодействовать только с определёнными клетками, B-лимфоцитами.  ']\n"
     ]
    }
   ],
   "source": [
    "v1 = []\n",
    "# Собираем примеры из таблицы\n",
    "for v in sheet.iter_rows(min_row=2, min_col=21, max_col=21, max_row=1001, values_only=True):\n",
    "    if v[0] == None:\n",
    "        v1.append('')\n",
    "    else:\n",
    "        v1.append(v[0].replace(u'\\xa0', ' '))\n",
    "v2 = []\n",
    "for v in sheet.iter_rows(min_row=2, min_col=22, max_col=22, max_row=1001, values_only=True):\n",
    "    if v[0] == None:\n",
    "        v2.append('')\n",
    "    else:\n",
    "        item = v[0].replace(u'\\xa0', ' ')\n",
    "        item = re.sub('\\[.+\\]', '', item)\n",
    "        item = re.sub(r'\\b[Тт]о есть', r'тоесть', item) # \\b убирает случаи что есть\n",
    "        item = re.sub(r'\\b[Аа] именно', r'аименно', item)\n",
    "        item = re.sub(r'\\b[Тт]ак как', r'таккак', item)\n",
    "        item = re.sub(r'\\b[Пп]отому что', r'потомучто', item)\n",
    "        v2.append(item)\n",
    "values = [v1[i]+v2[i] for i in range(1000)]\n",
    "print(values[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Парсим предложения\n",
    "def parser(string):\n",
    "    punctuation = (list(\"\"\",;:”$%^&*№()_—=+|[]{}\\\"/<>`~±§«»°1234567890\"\"\")\n",
    "               + ['- ', ' -', ' - ', '― ', ' ―', ' ― ', \" '\", \"' \"])\n",
    "    string = string.lower()\n",
    "    for ch in list('.…!?'): # Разбиваем примеры на предложения\n",
    "        string = string.replace(ch, '\\n')\n",
    "    for char in punctuation:\n",
    "        string = string.replace(char, ' ')\n",
    "    lemmas = m.lemmatize(string)\n",
    "    parsed = ''.join(lemmas)\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [parser(example) for example in values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Записываем в файл лемматизированные предложения\n",
    "with open('21_century.txt', 'w', encoding='utf-8') as fw:\n",
    "    for i in data:\n",
    "        fw.write(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Парсер корпуса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нужно:\n",
    "1. Соединить слова *то есть, а именно, потому что, так как*\n",
    "2. Убрать омонимию *если, то; не, а*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Volumes/Анна/Corpora/stable/disamb/corpora/lines_1m_to_3m.pickle', 'rb') as f:\n",
    "    data_13 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Volumes/Анна/Corpora/stable/disamb/corpora/lines_3000000.pickle', 'rb') as f:\n",
    "    data_3 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "BigData = data_13 + data_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Видим точку не в т.к., т.д. или т.е. - новое предложение\n",
    "text = []  # Список предложений\n",
    "sentence = []  # Обновляемый список слов в предложении\n",
    "k = -1  # Счетчик позиции. Вот если у нас \"т\" из т.е., мы можем не обрабатывать следующие 3 символа\n",
    "for i in range(len(BigData)-2):\n",
    "    if i <= k:  # Если счетчик больше, то мы уже учли это слово\n",
    "        continue\n",
    "    else:\n",
    "        k = -1\n",
    "    if not (BigData[i][0] == '!' or BigData[i][0] == '?' or BigData[i][0] == '.' or BigData[i][0] == '…') :\n",
    "        if (BigData[i][0] == 'то' or BigData[i][0] == 'То') and BigData[i+1][0] == 'есть':\n",
    "            sentence.append('тоесть')\n",
    "            k = i + 1  # Не смотрим на следующее \"есть\"\n",
    "        elif (BigData[i][0] == 'так' or BigData[i][0] == 'Так') and BigData[i+1][0] == 'как':\n",
    "            sentence.append('таккак')\n",
    "            k = i + 1\n",
    "        elif (BigData[i][0] == 'потому' or BigData[i][0] == 'Потому') and BigData[i+1][0] == 'что':\n",
    "            sentence.append('потомучто')\n",
    "            k = i + 1\n",
    "        elif (BigData[i][0] == 'А' or BigData[i][0] == 'А') and BigData[i+1][0] == 'именно':\n",
    "            sentence.append('аименно')\n",
    "            k = i + 1\n",
    "        elif (BigData[i][0] == 'т' or BigData[i][0] == 'Т') and BigData[i+1][0] == '.' and BigData[i+2][0] == 'е':\n",
    "            sentence.append('тоесть')\n",
    "            k = i + 3 \n",
    "        elif (BigData[i][0] == 'т' or BigData[i][0] == 'Т') and BigData[i+1][0] == '.' and BigData[i+2][0] == 'к':\n",
    "            sentence.append('таккак')\n",
    "            k = i + 3\n",
    "        else:\n",
    "            sentence.append(BigData[i][2])\n",
    "    elif not (BigData[i+1][0] == 'п' or BigData[i+1][0] == 'д'):  # Если у нас точка, и она не входит в \"т.п.\" и \"д.р.\"\n",
    "        text.append(sentence)\n",
    "        sentence = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lems.txt', 'w') as fw:\n",
    "    for i in text:\n",
    "        fw.write(' '.join(i).lower())\n",
    "        fw.write('/n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'lems.txt'\n",
    "data = gensim.models.word2vec.LineSentence(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sbow = gensim.models.Word2Vec(data, size=300, window=5, min_count=5, iter=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "model_sbow.init_sims(replace=True)\n",
    "model_path = \"to.bin\"\n",
    "\n",
    "print(\"Saving model...\")\n",
    "model_sbow.wv.save_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('и', 0.6347854733467102),\n",
       " ('в', 0.6344821453094482),\n",
       " ('таккак', 0.6324251890182495),\n",
       " ('быть', 0.6278139352798462),\n",
       " ('который', 0.6256133317947388),\n",
       " ('но', 0.6248376369476318),\n",
       " ('а', 0.6205902695655823),\n",
       " ('весь', 0.6196562051773071),\n",
       " ('хотя', 0.6196523904800415),\n",
       " ('если', 0.618808388710022),\n",
       " ('как', 0.6176967024803162),\n",
       " ('для', 0.6171594262123108),\n",
       " ('например', 0.6157851815223694),\n",
       " ('значит', 0.6156123876571655),\n",
       " ('либо', 0.6148117184638977),\n",
       " ('же', 0.6146326661109924),\n",
       " ('установить', 0.614324152469635),\n",
       " ('высокий', 0.6116353273391724),\n",
       " ('иметь', 0.6106708645820618),\n",
       " ('при', 0.6098961234092712),\n",
       " (',', 0.6087378263473511),\n",
       " ('или', 0.6086727976799011),\n",
       " ('причем', 0.60835200548172),\n",
       " ('этот', 0.6072604060173035),\n",
       " ('потомучто', 0.6063065528869629),\n",
       " ('где', 0.6062798500061035),\n",
       " ('это', 0.6058995723724365),\n",
       " ('–', 0.6058865189552307),\n",
       " (')', 0.6044421792030334),\n",
       " ('на', 0.604044497013092),\n",
       " ('без', 0.6039661169052124),\n",
       " ('не', 0.6037483215332031),\n",
       " ('кстати', 0.6035818457603455),\n",
       " ('\"', 0.6004090309143066),\n",
       " ('-', 0.5990405082702637),\n",
       " ('вот', 0.597612738609314),\n",
       " ('здесь', 0.596852719783783),\n",
       " ('красный', 0.596537172794342),\n",
       " ('человек', 0.5963411331176758),\n",
       " ('с', 0.5963306427001953),\n",
       " ('что', 0.5959026217460632),\n",
       " ('то', 0.5925649404525757),\n",
       " ('reply', 0.5924988389015198),\n",
       " ('действовать', 0.5917878746986389),\n",
       " ('учебный', 0.5916445851325989),\n",
       " ('количество', 0.590801477432251),\n",
       " ('начинать', 0.5906822085380554),\n",
       " ('поэтому', 0.5905004739761353),\n",
       " ('требовать', 0.5903305411338806),\n",
       " ('разумеется', 0.590276837348938)]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sbow.wv.most_similar_cosmul(\"тоесть\", topn=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 'я здесь самый умный тоесть как  самый умный  недоумевать'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = l.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('лучший', 3.6221896e-05),\n",
       " ('главное', 3.6005662e-05),\n",
       " ('дело', 3.525131e-05),\n",
       " ('важный', 3.524584e-05),\n",
       " ('умный', 3.4789413e-05),\n",
       " ('красивый', 3.4572855e-05),\n",
       " ('интересный', 3.426596e-05),\n",
       " ('слово', 3.350955e-05),\n",
       " ('смелый', 3.3074713e-05),\n",
       " ('правильный', 3.3052063e-05)]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_sbow.predict_output_word(l) # Это я не очень поняла как работает, но еще особенно не разбиралась"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проблемы:\n",
    "1. Не учитываем омонимию (*Если модель заработает, **то есть** шанс, что все будет хорошо.* и т.п.)\n",
    "2. Разбиение на предложения портят штуки типа г.Москва, 2000гг. и др.\n",
    "3. Нужно ли оставлять знаки препинания? Например, тире это что-то типа равнозначности\n",
    "4. Нужно ли разделять случаи *То есть* и *то есть*?\n",
    "5. Что-то явно не то с парсингом моих примеров, но я пока не вижу что\n",
    "6. А сколько вообще ближайших соседей адекватное число?\n",
    "7. Косинусная близость соседей примерно 0.3, это очень мало, что делать? Тут я поменяла метод измерения, опять же не разобравшись в нем (Простите, не успела пока)\n",
    "8. А еще не учтены случаи знаков препинания типа *??!!!!*\n",
    "9. Все-таки пока не видно дискурсивного *то есть*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дальше я пробовала работать с другими моделями\n",
    "\n",
    "Тут все плоховато"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW с моими примерами"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ничего не работает из-за плохо обработанных моих примеров. Я еще поработаю над ними."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"corp_table.txt\", \"w\") as fw:\n",
    "    for i in text:\n",
    "        fw.write(' '.join(i).lower())\n",
    "        fw.write('/n')\n",
    "    for i in data:\n",
    "        fw.write(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'corp_table.txt'\n",
    "data = gensim.models.word2vec.LineSentence(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cbow_full = gensim.models.Word2Vec(data, size=300, window=5, min_count=5, iter=50, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "model_cbow_full.init_sims(replace=True)\n",
    "model_path = \"to.bin\"\n",
    "\n",
    "print(\"Saving model...\")\n",
    "model_cbow_full.wv.save_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ароматно', 0.33030757308006287),\n",
       " ('загрязнение', 0.3236773610115051),\n",
       " ('общенародный', 0.31093573570251465),\n",
       " ('суфражистка', 0.30130282044410706),\n",
       " ('израсходовать', 0.29774612188339233),\n",
       " ('олить', 0.2936965227127075),\n",
       " ('цивилизованно', 0.2917793393135071),\n",
       " ('11-13', 0.29002130031585693),\n",
       " ('поглазеть', 0.28996801376342773),\n",
       " ('обратный', 0.28883910179138184),\n",
       " ('начитать', 0.2886679768562317),\n",
       " ('дж/nверди', 0.28675782680511475),\n",
       " ('доллар/nсейчас', 0.28536850214004517),\n",
       " ('прогрессивнее', 0.28291743993759155),\n",
       " ('мотивировка', 0.28272777795791626),\n",
       " ('пропорциональный', 0.2820762097835541),\n",
       " ('сужение', 0.279745876789093),\n",
       " ('возбудиться', 0.2797427177429199),\n",
       " ('dds', 0.2786122262477875),\n",
       " ('всесоюзный', 0.2781897783279419),\n",
       " (')/nсоответственно', 0.27817827463150024),\n",
       " ('передел', 0.2773246765136719),\n",
       " ('таргум', 0.2769983410835266),\n",
       " ('детерминизм', 0.27667146921157837),\n",
       " ('заметнее', 0.2763804793357849),\n",
       " ('4-ого', 0.27579981088638306),\n",
       " ('таня/n-', 0.27409252524375916),\n",
       " ('трудотерапия', 0.2738110423088074),\n",
       " ('спг', 0.27371978759765625),\n",
       " ('поэтому', 0.2731184959411621),\n",
       " ('зальчик', 0.2726001739501953),\n",
       " ('отвлечение', 0.2720457911491394),\n",
       " ('десятикратно', 0.2715880274772644),\n",
       " ('новoгo', 0.2707105576992035),\n",
       " ('погрузка', 0.2700168192386627),\n",
       " ('машинка/nа', 0.2692686915397644),\n",
       " ('народный', 0.26766470074653625),\n",
       " ('клан', 0.26725995540618896),\n",
       " ('вливать', 0.2667360007762909),\n",
       " ('стихия', 0.26632049679756165),\n",
       " (')/n-', 0.26616179943084717),\n",
       " ('ковровый', 0.2654074430465698),\n",
       " ('последовательный', 0.2650317847728729),\n",
       " ('вахта', 0.26450538635253906),\n",
       " ('поддержание', 0.26438117027282715),\n",
       " ('канибализм', 0.2641395628452301),\n",
       " ('покориться', 0.2639894187450409),\n",
       " ('изворачиваться', 0.26369816064834595),\n",
       " ('9000', 0.26329341530799866),\n",
       " ('связка', 0.26244938373565674)]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_cbow_full.wv.most_similar(\"тоесть\", topn=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ничего не работает, потому что это skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'lems.txt'\n",
    "data = gensim.models.word2vec.LineSentence(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to = gensim.models.Word2Vec(data, size=300, window=5, min_count=5, iter=50, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "model_to.init_sims(replace=True)\n",
    "model_path = \"to.bin\"\n",
    "\n",
    "print(\"Saving model...\")\n",
    "model_to.wv.save_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ароматно', 0.34522080421447754),\n",
       " ('\"/nдолгин', 0.3146291971206665),\n",
       " ('суфражистка', 0.30915361642837524),\n",
       " ('выключатель', 0.30832329392433167),\n",
       " ('зальчик', 0.30427107214927673),\n",
       " ('комплексный', 0.3041404187679291),\n",
       " ('израсходовать', 0.3035739064216614),\n",
       " ('ошибка/nв', 0.3016357421875),\n",
       " ('350', 0.2975699305534363),\n",
       " (')/nсоответственно', 0.2966846823692322),\n",
       " ('таня/n-', 0.2957918047904968),\n",
       " ('таргум', 0.29400986433029175),\n",
       " ('цивилизованно', 0.2903851866722107),\n",
       " ('олить', 0.29026418924331665),\n",
       " ('запущенность', 0.2897460460662842),\n",
       " ('ребяческий', 0.28849926590919495),\n",
       " ('отвественность', 0.2881307005882263),\n",
       " ('передел', 0.28570765256881714),\n",
       " ('загрязнение', 0.2841710150241852),\n",
       " ('отождествляться', 0.283867746591568),\n",
       " ('inclusive', 0.28291207551956177),\n",
       " ('изворачиваться', 0.28290051221847534),\n",
       " ('кутч', 0.282699853181839),\n",
       " ('существительное', 0.2816200852394104),\n",
       " ('переложение', 0.27822941541671753),\n",
       " ('деньги/nэто', 0.2775688171386719),\n",
       " ('отваливать', 0.2775288224220276),\n",
       " ('начитать', 0.27723604440689087),\n",
       " ('уровень/nи', 0.27698850631713867),\n",
       " ('бильярдная', 0.27607184648513794),\n",
       " ('дж/nверди', 0.27595189213752747),\n",
       " ('80-100', 0.27529624104499817),\n",
       " ('жигульский', 0.27461355924606323),\n",
       " ('сужение', 0.27137359976768494),\n",
       " ('вливать', 0.2712633013725281),\n",
       " ('поздравлялка', 0.2712431252002716),\n",
       " ('возбудиться', 0.27045658230781555),\n",
       " ('ковровый', 0.26711905002593994),\n",
       " ('получаться/nи', 0.26698988676071167),\n",
       " ('гадюка', 0.26684918999671936),\n",
       " ('охранение', 0.2664738893508911),\n",
       " ('мотивировка', 0.2660151720046997),\n",
       " ('всесоюзный', 0.2657342553138733),\n",
       " ('15000', 0.2650810480117798),\n",
       " ('пресечь', 0.2646418809890747),\n",
       " ('слабее', 0.263579785823822),\n",
       " ('шире', 0.2630169689655304),\n",
       " ('оранжевый/nи', 0.26265645027160645),\n",
       " ('1935', 0.2625850439071655),\n",
       " ('гулянье', 0.2624382972717285)]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_to.wv.most_similar(\"тоесть\", topn=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут все совсем странно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lems.txt') as fh:\n",
    "    lines = fh.readlines()\n",
    "lines = [line.split() for line in lines]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 30 training epochs with 4 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n"
     ]
    }
   ],
   "source": [
    "#importing the glove library\n",
    "from glove import Corpus, Glove\n",
    "# creating a corpus object\n",
    "corpus = Corpus() \n",
    "#training the corpus to generate the co occurence matrix which is used in GloVe\n",
    "corpus.fit(lines, window=10)\n",
    "#creating a Glove object which will use the matrix created in the above lines to create embeddings\n",
    "#We can set the learning rate as it uses Gradient Descent and number of components\n",
    "glove = Glove(no_components=5, learning_rate=0.05)\n",
    " \n",
    "glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "glove.save('glove.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(')/nа', 0.9989536978920346), ('вчера', 0.9987213646983383), ('тема', 0.9986575014778142), ('year', 0.9981276952849215)]\n"
     ]
    }
   ],
   "source": [
    "print(glove.most_similar('тоесть'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaGram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во втором значении модель говорит, что *то есть* похоже на какие-то редкие полнозначные слова. В первом значении немного получше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 2020-01-25 13:22:59,600 Building dictionary...\n",
      "[INFO] 2020-01-25 13:28:27,833 Done! 21286 words.\n",
      "[INFO] 2020-01-25 13:34:48,041 1.67% -6.6546 0.0246 1.2/3.0 0.17 kwords/sec\n",
      "[INFO] 2020-01-25 13:34:51,640 3.33% -6.4787 0.0242 1.5/4.0 17.76 kwords/sec\n",
      "[INFO] 2020-01-25 13:34:55,177 5.00% -6.3445 0.0238 1.7/5.0 18.10 kwords/sec\n",
      "[INFO] 2020-01-25 13:34:58,666 6.67% -6.2380 0.0233 1.7/5.0 18.34 kwords/sec\n",
      "[INFO] 2020-01-25 13:35:02,195 8.33% -6.1489 0.0229 1.8/5.0 18.14 kwords/sec\n",
      "[INFO] 2020-01-25 13:35:06,370 10.00% -6.0709 0.0225 1.8/5.0 15.33 kwords/sec\n",
      "[INFO] 2020-01-25 13:35:09,682 11.66% -6.0008 0.0221 1.8/5.0 19.32 kwords/sec\n",
      "[INFO] 2020-01-25 13:35:13,123 13.33% -5.9372 0.0217 1.9/5.0 18.60 kwords/sec\n",
      "[INFO] 2020-01-25 13:35:16,715 15.00% -5.8787 0.0213 1.9/5.0 17.82 kwords/sec\n",
      "[INFO] 2020-01-25 13:35:20,265 16.66% -5.8252 0.0208 1.9/5.0 18.03 kwords/sec\n",
      "[INFO] 2020-01-25 13:35:23,521 18.33% -5.7758 0.0204 2.0/5.0 19.65 kwords/sec\n",
      "[INFO] 2020-01-25 13:35:26,827 20.00% -5.7302 0.0200 2.0/5.0 19.36 kwords/sec\n",
      "[INFO] 2020-01-25 13:35:30,133 21.66% -5.6881 0.0196 2.0/5.0 19.36 kwords/sec\n",
      "[INFO] 2020-01-25 13:35:33,392 23.33% -5.6491 0.0192 2.0/5.0 19.64 kwords/sec\n",
      "[INFO] 2020-01-25 13:35:36,667 24.99% -5.6130 0.0188 2.0/5.0 19.54 kwords/sec\n",
      "[INFO] 2020-01-25 13:35:39,897 26.66% -5.5794 0.0183 2.0/5.0 19.82 kwords/sec\n",
      "[INFO] 2020-01-25 13:35:43,101 28.33% -5.5483 0.0179 2.0/5.0 19.97 kwords/sec\n",
      "[INFO] 2020-01-25 13:35:46,271 29.99% -5.5192 0.0175 2.0/5.0 20.19 kwords/sec\n",
      "[INFO] 2020-01-25 13:35:49,423 31.66% -5.4922 0.0171 2.0/5.0 20.31 kwords/sec\n",
      "[INFO] 2020-01-25 13:35:52,554 33.33% -5.4669 0.0167 2.0/5.0 20.44 kwords/sec\n",
      "[INFO] 2020-01-25 13:35:55,702 34.99% -5.4432 0.0163 2.1/5.0 20.33 kwords/sec\n",
      "[INFO] 2020-01-25 13:35:58,839 36.66% -5.4210 0.0158 2.1/5.0 20.40 kwords/sec\n",
      "[INFO] 2020-01-25 13:36:01,956 38.33% -5.4001 0.0154 2.1/5.0 20.53 kwords/sec\n",
      "[INFO] 2020-01-25 13:36:05,073 39.99% -5.3804 0.0150 2.1/5.0 20.53 kwords/sec\n",
      "[INFO] 2020-01-25 13:36:08,209 41.66% -5.3619 0.0146 2.1/5.0 20.41 kwords/sec\n",
      "[INFO] 2020-01-25 13:36:11,315 43.32% -5.3444 0.0142 2.1/5.0 20.61 kwords/sec\n",
      "[INFO] 2020-01-25 13:36:14,419 44.99% -5.3279 0.0138 2.1/5.0 20.61 kwords/sec\n",
      "[INFO] 2020-01-25 13:36:17,524 46.66% -5.3122 0.0133 2.1/5.0 20.62 kwords/sec\n",
      "[INFO] 2020-01-25 13:36:20,618 48.32% -5.2974 0.0129 2.1/5.0 20.69 kwords/sec\n",
      "[INFO] 2020-01-25 13:36:23,707 49.99% -5.2833 0.0125 2.1/5.0 20.71 kwords/sec\n",
      "[INFO] 2020-01-25 13:36:26,781 51.66% -5.2699 0.0121 2.1/5.0 20.82 kwords/sec\n",
      "[INFO] 2020-01-25 13:36:29,861 53.32% -5.2571 0.0117 2.1/5.0 20.78 kwords/sec\n",
      "[INFO] 2020-01-25 13:36:32,943 54.99% -5.2449 0.0113 2.1/5.0 20.77 kwords/sec\n",
      "[INFO] 2020-01-25 13:36:36,005 56.65% -5.2333 0.0108 2.1/5.0 20.90 kwords/sec\n",
      "[INFO] 2020-01-25 13:36:39,080 58.32% -5.2222 0.0104 2.1/5.0 20.81 kwords/sec\n",
      "[INFO] 2020-01-25 13:36:42,162 59.99% -5.2117 0.0100 2.1/5.0 20.77 kwords/sec\n",
      "[INFO] 2020-01-25 13:36:45,231 61.65% -5.2015 0.0096 2.1/5.0 20.86 kwords/sec\n",
      "[INFO] 2020-01-25 13:36:48,290 63.32% -5.1918 0.0092 2.1/5.0 20.92 kwords/sec\n",
      "[INFO] 2020-01-25 13:36:51,349 64.99% -5.1825 0.0088 2.1/5.0 20.92 kwords/sec\n",
      "[INFO] 2020-01-25 13:36:54,417 66.65% -5.1736 0.0083 2.1/5.0 20.86 kwords/sec\n",
      "[INFO] 2020-01-25 13:36:57,484 68.32% -5.1651 0.0079 2.1/5.0 20.87 kwords/sec\n",
      "[INFO] 2020-01-25 13:37:00,545 69.99% -5.1569 0.0075 2.1/5.0 20.91 kwords/sec\n",
      "[INFO] 2020-01-25 13:37:03,630 71.65% -5.1491 0.0071 2.1/5.0 20.74 kwords/sec\n",
      "[INFO] 2020-01-25 13:37:06,692 73.32% -5.1415 0.0067 2.2/5.0 20.91 kwords/sec\n",
      "[INFO] 2020-01-25 13:37:09,762 74.98% -5.1343 0.0063 2.2/5.0 20.85 kwords/sec\n",
      "[INFO] 2020-01-25 13:37:12,818 76.65% -5.1274 0.0058 2.2/5.0 20.94 kwords/sec\n",
      "[INFO] 2020-01-25 13:37:16,017 78.32% -5.1208 0.0054 2.2/5.0 20.01 kwords/sec\n",
      "[INFO] 2020-01-25 13:37:19,289 79.98% -5.1144 0.0050 2.2/5.0 19.56 kwords/sec\n",
      "[INFO] 2020-01-25 13:37:22,345 81.65% -5.1083 0.0046 2.2/5.0 20.94 kwords/sec\n",
      "[INFO] 2020-01-25 13:37:25,572 83.32% -5.1025 0.0042 2.2/5.0 19.84 kwords/sec\n",
      "[INFO] 2020-01-25 13:37:28,633 84.98% -5.0969 0.0038 2.2/5.0 20.90 kwords/sec\n",
      "[INFO] 2020-01-25 13:37:31,732 86.65% -5.0917 0.0033 2.2/5.0 20.66 kwords/sec\n",
      "[INFO] 2020-01-25 13:37:34,817 88.31% -5.0866 0.0029 2.2/5.0 20.74 kwords/sec\n",
      "[INFO] 2020-01-25 13:37:38,001 89.98% -5.0819 0.0025 2.2/5.0 20.10 kwords/sec\n",
      "[INFO] 2020-01-25 13:37:41,079 91.65% -5.0774 0.0021 2.3/5.0 20.79 kwords/sec\n",
      "[INFO] 2020-01-25 13:37:44,181 93.31% -5.0732 0.0017 2.3/5.0 20.63 kwords/sec\n",
      "[INFO] 2020-01-25 13:37:47,416 94.98% -5.0693 0.0013 2.3/5.0 19.78 kwords/sec\n",
      "[INFO] 2020-01-25 13:37:50,570 96.65% -5.0658 0.0008 2.3/5.0 20.30 kwords/sec\n",
      "[INFO] 2020-01-25 13:37:53,865 98.31% -5.0626 0.0004 2.4/5.0 19.42 kwords/sec\n",
      "[INFO] 2020-01-25 13:37:57,160 99.98% -5.0600 0.0000 2.4/5.0 19.42 kwords/sec\n",
      "[INFO] 2020-01-25 13:37:57,202 100.00% -5.0597 0.0000 2.5/5.0 19.24 kwords/sec\n"
     ]
    }
   ],
   "source": [
    "! adagram-train lems.txt out1.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import adagram\n",
    "vm = adagram.VectorModel.load('out1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.9999310392386732)]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm.word_sense_probs('тоесть')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(':', 4, 0.4486275),\n",
       " ('мыслить', 0, 0.4284525),\n",
       " ('к', 2, 0.4283172),\n",
       " ('человеческий', 0, 0.42445052),\n",
       " ('тренд', 0, 0.42221954),\n",
       " ('некорректно', 0, 0.41890633),\n",
       " ('-', 3, 0.4168862),\n",
       " ('уничтожать', 0, 0.4129591),\n",
       " ('у', 0, 0.40553477),\n",
       " ('в', 1, 0.39676043)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vm.sense_neighbors('тоесть', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
